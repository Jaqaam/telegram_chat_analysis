{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0015218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import nltk\n",
    "import wordcloud\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_messages_to_dataframe(html_file, year=None):\n",
    "    \"\"\"\n",
    "    Parse messages from the HTML file and load them into a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        html_file (str): Path to the exported Telegram chat HTML file.\n",
    "        year (int): Optional. Filter messages by year.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the parsed messages.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store message data\n",
    "    data = []\n",
    "\n",
    "    # Loop through all 'message default clearfix' divs\n",
    "    for message in soup.find_all('div', class_='message default clearfix'):\n",
    "        # Extract the timestamp from the 'pull_right date details' div\n",
    "        date_div = message.find('div', class_='pull_right date details')\n",
    "        if date_div and 'title' in date_div.attrs:\n",
    "            date_str = date_div['title']  # Example: \"20.01.2024 20:02:09 UTC+01:00\"\n",
    "            try:\n",
    "                message_date = datetime.strptime(date_str, \"%d.%m.%Y %H:%M:%S UTC%z\")\n",
    "            except ValueError:\n",
    "                continue  # Skip if the date format is unexpected\n",
    "\n",
    "            # Filter by year (if specified)\n",
    "            if year and message_date.year != year:\n",
    "                continue\n",
    "\n",
    "            # Extract the sender's name from the 'from_name' div\n",
    "            from_name_div = message.find('div', class_='from_name')\n",
    "            sender = from_name_div.text.strip() if from_name_div else \"(Unknown)\"\n",
    "\n",
    "            # Extract the message text from the 'text' div\n",
    "            text_div = message.find('div', class_='text')\n",
    "            message_text = text_div.text.strip() if text_div else \"(No text)\"\n",
    "            \n",
    "            #extract reactions\n",
    "            reactions = \"\"\n",
    "            for reaction in message.find_all('div', class_='reactions'):\n",
    "            #reactions_div = message.find\n",
    "                emoji_text = reaction.find('div',class_='emoji').text.strip()\n",
    "                emoji_owner = reaction.find('div', class_='initials')['title']\n",
    "                reactions = reactions+f\"{emoji_text},{emoji_owner}\"\n",
    "            \n",
    "            #extract photo sender\n",
    "            photos =[]\n",
    "            for photo in message.find_all('div', class_=\"media_wrap clearfix\"):\n",
    "                a_tag= photo.find('a', class_=\"photo_wrap clearfix pull_left\")\n",
    "                photo_href = a_tag['href'] if a_tag else None\n",
    "                body_div = a_tag.find_parent('div', class_='body')  if a_tag else None\n",
    "                photo_owner = body_div.find('div', class_='from_name').text if body_div else None\n",
    "                photos.append([photo_owner,photo_href])\n",
    "                #print(photo_owner, photo_href)\n",
    "                #photo_owner = photo.find()\n",
    "                #print(photo_href)\n",
    "            # Append the message data to the list\n",
    "            data.append({\n",
    "                'timestamp': message_date,\n",
    "                'sender': sender,\n",
    "                'message': message_text,\n",
    "                'reactions': reactions, \n",
    "                'photo': photos\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "html_file = \"path/to/dir\"\n",
    "#year = 2024  # The year to filter messages by\n",
    "author = \"Jav\"\n",
    "#filtered_messages = extract_messages_by_year(html_file, year)\n",
    "#filtered_messages = extract_messages_by_author(html_file, author)\n",
    "df = load_messages_to_dataframe(html_file, year=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['photo'].apply(lambda x: len(x) > 0), 'photo']#[47]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(timestamp):\n",
    "    result = timestamp.date()\n",
    "    return result\n",
    "\n",
    "def get_time(timestamp):\n",
    "    result = timestamp.time()\n",
    "    return result\n",
    "\n",
    "\n",
    "def date_to_integer(date_obj):\n",
    "    \"\"\"\n",
    "    Converts a date object to an integer representing the number of days\n",
    "    elapsed since January 1st of the same year.\n",
    "    \n",
    "    Args:\n",
    "    - date_obj (datetime.date): The date object to convert.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Number of days since January 1st.\n",
    "    \"\"\"\n",
    "    date_obj = date_obj.date()\n",
    "    # Reference point: January 1st of the same year\n",
    "    reference_date = date(date_obj.year, 1, 1)\n",
    "    # Calculate the difference in days\n",
    "    days_elapsed = (date_obj - reference_date).days +1\n",
    "    return days_elapsed\n",
    "\n",
    "\n",
    "months = pd.Series(['Jan', 'Feb', 'MÃ¤r', 'Apr', 'Mai', 'Jun', \n",
    "'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dez'])\n",
    "\n",
    "df[\"date\"] = df[\"timestamp\"].apply(get_date)    \n",
    "df[\"time\"] = df[\"timestamp\"].apply(get_time)    \n",
    "df[\"month\"] = df[\"timestamp\"].apply(lambda x: x.month).astype('category')\n",
    "df[\"day\"] = df[\"timestamp\"].apply(lambda x: x.day)    \n",
    "df[\"date_int\"] = df[\"timestamp\"].apply(date_to_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a52988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "language=\"german\"\n",
    "df_jav = df.loc[df[\"sender\"] == \"Jav\"]\n",
    "text_jav = df_jav['message'].str.cat(sep='').lower()\n",
    "print(type(text_jav),len(text_jav))\n",
    "print(text_jav)\n",
    "\n",
    "sentences = nltk.sent_tokenize(text_jav,language='german')\n",
    "\n",
    "\n",
    "print(len(sentences),sentences)\n",
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4297a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get most common words off tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52250601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import de_core_news_lg\n",
    "import de_core_news_md\n",
    "from collections import Counter\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "all_tokens = \"\"\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    #lemmas = [token.lemma_ for token in doc]\n",
    "    #print(\"Lemmas:\", lemmas)\n",
    "    \n",
    "    # Get the tokens\n",
    "    tokens = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop and not token.text==\"mal\"\n",
    "        and not token.text==\"no\" and not token.text==\"text\"\n",
    "    ]    \n",
    "    print(\"tokens \",tokens)\n",
    "    all_tokens = all_tokens + \" \".join(tokens)\n",
    "    \n",
    "    # Update the word frequencies\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "    # Find the most common words\n",
    "most_common_words = word_counter.most_common(50)  # Change 10 to any number of top words you want\n",
    "print(\"Most common words:\", most_common_words)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9700fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which user:\n",
    "text = text_jav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "#text = '\\n'.join(text_jens)\n",
    "#print(text)\n",
    "doc = nlp(text)\n",
    "\n",
    "# Generating a word cloud with the adjetives of the story\n",
    "words = ' '.join(\n",
    "    [ \n",
    "     token.text for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop \n",
    "        and not token.text==\"mal\"\n",
    "        and not token.text==\"no\" and not token.text==\"text\"\n",
    "    ])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9fb2a",
   "metadata": {},
   "source": [
    "# general wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate word cloud\n",
    "wc = wordcloud.WordCloud().generate(words)\n",
    "\n",
    "# Showing word cloud\n",
    "plt.figure(figsize=(15, 15),dpi=500)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d51a52",
   "metadata": {},
   "source": [
    "# adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(\n",
    "    [ \n",
    "        token.norm_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.text==\"mal\" and not token.is_stop\n",
    "        and not token.text==\"no\" and not token.text==\"text\" and token.pos_ in ['ADJ'] and not token.text.lower()==\"janosch\"\n",
    "    ])\n",
    "\n",
    "\n",
    "# Generate word cloud\n",
    "wc = wordcloud.WordCloud().generate(words)\n",
    "\n",
    "#print(doc)\n",
    "\n",
    "# Showing word cloud\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(\n",
    "    [ \n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "        and not token.text==\"no\" and not token.text==\"text\" and token.pos_ in ['VERB']\n",
    "    ])\n",
    "\n",
    "# Generate word cloud\n",
    "wc = wordcloud.WordCloud().generate(words)\n",
    "\n",
    "# Showing word cloud\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d7f5b",
   "metadata": {},
   "source": [
    "# nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(\n",
    "    [ \n",
    "        token.norm_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "        and not token.text==\"no\" and not token.text==\"text\" and token.pos_ in ['NOUN']\n",
    "    ])\n",
    "\n",
    "# Generate word cloud\n",
    "wc = wordcloud.WordCloud().generate(words)\n",
    "\n",
    "# Showing word cloud\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395943e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(\n",
    "    [ \n",
    "        token.norm_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space #and not token.is_stop\n",
    "        and not token.text==\"no\" and not token.text==\"text\" and token.pos_ in ['PROPN']\n",
    "    ])\n",
    "tokens = [ \n",
    "        token.norm_\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop\n",
    "        and not token.text==\"no\" and not token.text==\"text\" and token.pos_ in ['PROPN']\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "word_counter = Counter()\n",
    "word_counter.update(tokens)\n",
    "most_common_words = word_counter.most_common(50)\n",
    "#print(\"Most common words:\", most_common_words)\n",
    "\n",
    "# Generate word cloud\n",
    "wc = wordcloud.WordCloud().generate(words)\n",
    "\n",
    "# Showing word cloud\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d62c14",
   "metadata": {},
   "source": [
    "# emoji analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e1d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Show used emojis\n",
    "emoji_user_dict = {}\n",
    "for user in user_names:\n",
    "    df_emoji = df.loc[df[\"sender\"] == user]\n",
    "    text = df_emoji['message'].str.cat(sep='\\n')\n",
    "    out = (pd.DataFrame(emoji.emoji_list(text)).value_counts('emoji')\n",
    "             .rename_axis('Smiley').rename('Count').reset_index()\n",
    "             .assign(Type=lambda x: x['Smiley'].apply(emoji.demojize)))\n",
    "    results = out.iloc[:10]\n",
    "    emoji_user_dict[user] = results.to_numpy()\n",
    "\n",
    "print(emoji_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot used emojis of each user\n",
    "\n",
    "top_num = 10\n",
    "\n",
    "\n",
    "top_emojis_per_user = {}\n",
    "for user, smileys in emoji_user_dict.items():\n",
    "    # Sort emojis by count (descending), then take the top 10\n",
    "    sorted_smileys = sorted(smileys, key=lambda x: int(x[1]), reverse=True)\n",
    "    top_emojis_per_user[user] = sorted_smileys[:top_num]\n",
    "\n",
    "\n",
    "unique_smileys = set() #in case users have share top 10 emojis\n",
    "for smileys in top_emojis_per_user.values():\n",
    "    for _, _, demojized in smileys:\n",
    "        unique_smileys.add(demojized)\n",
    "unique_smileys = list(unique_smileys)  # Convert to list for indexing\n",
    "\n",
    "users = list(emoji_user_dict.keys())\n",
    "counts_matrix = {\n",
    "    user: [next((int(smiley[1]) for smiley in top_emojis_per_user[user] if smiley[2] == demojized), 0)\n",
    "           for demojized in unique_smileys]\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "\n",
    "# Plotting with specified colors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Color sequence: red, baby blue, green, pink, yellow\n",
    "colors = ['red', '#89CFF0', 'green', 'pink', 'yellow']\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    ax.plot(\n",
    "        unique_smileys,\n",
    "        counts_matrix[user],\n",
    "        marker='o',  # Markers for better visibility\n",
    "        label=user,\n",
    "        color=colors[i % len(colors)]  # Cycle through the color list\n",
    "    )\n",
    "\n",
    "\n",
    "ax.set_xlabel('Smileys (Demojinized)', fontsize=12)\n",
    "ax.set_ylabel('Anzahl Gesendet', fontsize=12)\n",
    "ax.set_title(f'Top {top_num} Smileys der Gruppe', fontsize=14)\n",
    "ax.legend(title='User', fontsize=10)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d995a9",
   "metadata": {},
   "source": [
    "# Reactions Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e593afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show emojis of users used in Telegram Reactions\n",
    "\n",
    "text = df['reactions'].str.cat(sep='\\n')\n",
    "out = (pd.DataFrame(emoji.emoji_list(text)).value_counts('emoji')\n",
    "         .rename_axis('Smiley').rename('Count').reset_index()\n",
    "         .assign(Type=lambda x: x['Smiley'].apply(emoji.demojize)))\n",
    "out.iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410ff69",
   "metadata": {},
   "source": [
    "# Messages Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show emojis send in Messages\n",
    "\n",
    "text = df['message'].str.cat(sep='\\n')\n",
    "out = (pd.DataFrame(emoji.emoji_list(text)).value_counts('emoji')\n",
    "         .rename_axis('Smiley').rename('Count').reset_index()\n",
    "         .assign(Type=lambda x: x['Smiley'].apply(emoji.demojize)))\n",
    "out.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33eed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
